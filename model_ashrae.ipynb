{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import gc\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to reduce memory usage https://www.kaggle.com/ihelon/simple-xgboost\n",
    "\n",
    "Have a look at https://www.kaggle.com/nroman/4-kfolds-lightgbm with in particular reduce_mem_usage_sd and lightgbm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.csdn.net/appleyuchi/article/details/100251308\n",
    "def memory_usage_mb(df, *args, **kwargs):\n",
    "    \"\"\"Dataframe memory usage in MB. \"\"\"\n",
    "    return df.memory_usage(*args, **kwargs).sum() / 1024**2\n",
    " \n",
    "\n",
    "def reduce_memory_usage(df, deep=True, verbose=True, categories=True):\n",
    "    # All types that we want to change for \"lighter\" ones.\n",
    "    # int8 and float16 are not include because we cannot reduce\n",
    "    # those data types.\n",
    "    # float32 is not include because float16 has too low precision.\n",
    "    numeric2reduce = [\"int16\", \"int32\", \"int64\", \"float64\"]\n",
    "    start_mem = 0\n",
    "    if verbose:\n",
    "        start_mem = memory_usage_mb(df, deep=deep)\n",
    " \n",
    "    for col, col_type in df.dtypes.iteritems():\n",
    "        best_type = None\n",
    "        if col_type == \"object\":\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "            best_type = \"category\"\n",
    "        elif col_type in numeric2reduce:\n",
    "            downcast = \"integer\" if \"int\" in str(col_type) else \"float\"\n",
    "            df[col] = pd.to_numeric(df[col], downcast=downcast)\n",
    "            best_type = df[col].dtype.name\n",
    "        # Log the conversion performed.\n",
    "        if verbose and best_type is not None and best_type != str(col_type):\n",
    "            print(f\"Column '{col}' converted from {col_type} to {best_type}\")\n",
    " \n",
    "    if verbose:\n",
    "        end_mem = memory_usage_mb(df, deep=deep)\n",
    "        diff_mem = start_mem - end_mem\n",
    "        percent_mem = 100 * diff_mem / start_mem\n",
    "        print(f\"Memory usage decreased from\"\n",
    "              f\" {start_mem:.2f}MB to {end_mem:.2f}MB\"\n",
    "              f\" ({diff_mem:.2f}MB, {percent_mem:.2f}% reduction)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/ Read the data for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read pickle\n",
      "read pickle successful\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>site_id</th>\n",
       "      <th>primary_use</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>year_built</th>\n",
       "      <th>floor_count</th>\n",
       "      <th>air_temperature</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>dew_temperature</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>7432</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.700012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>2720</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.700012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>5376</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.700012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>23685</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.700012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Education</td>\n",
       "      <td>116607</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1019.700012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  meter  meter_reading  site_id primary_use  square_feet  \\\n",
       "0            0      0            0.0        0   Education         7432   \n",
       "1            1      0            0.0        0   Education         2720   \n",
       "2            2      0            0.0        0   Education         5376   \n",
       "3            3      0            0.0        0   Education        23685   \n",
       "4            4      0            0.0        0   Education       116607   \n",
       "\n",
       "   year_built  floor_count  air_temperature  cloud_coverage  dew_temperature  \\\n",
       "0      2008.0          NaN             25.0             6.0             20.0   \n",
       "1      2004.0          NaN             25.0             6.0             20.0   \n",
       "2      1991.0          NaN             25.0             6.0             20.0   \n",
       "3      2002.0          NaN             25.0             6.0             20.0   \n",
       "4      1975.0          NaN             25.0             6.0             20.0   \n",
       "\n",
       "   precip_depth_1_hr  sea_level_pressure  wind_direction  wind_speed  \\\n",
       "0                NaN         1019.700012             0.0         0.0   \n",
       "1                NaN         1019.700012             0.0         0.0   \n",
       "2                NaN         1019.700012             0.0         0.0   \n",
       "3                NaN         1019.700012             0.0         0.0   \n",
       "4                NaN         1019.700012             0.0         0.0   \n",
       "\n",
       "   timestamp  \n",
       "0 2016-01-01  \n",
       "1 2016-01-01  \n",
       "2 2016-01-01  \n",
       "3 2016-01-01  \n",
       "4 2016-01-01  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data files\n",
    "def read_data(case='train'):\n",
    "    try:\n",
    "        print('read pickle')\n",
    "        df = pd.read_pickle(case + '.pkl')\n",
    "        print('read pickle successful')\n",
    "    except FileNotFoundError:\n",
    "        print('read data from source files')\n",
    "        df = pd.read_csv('data/{}.csv'.format(case))\n",
    "        df_meta = pd.read_csv('data/building_metadata.csv')\n",
    "        df_wh = pd.read_csv('data/weather_{}.csv'.format(case))\n",
    "\n",
    "        # merge datasets\n",
    "        print('merge datasets')\n",
    "        df = df.merge(df_meta, on='building_id', how='left').sort_values(['building_id', 'meter', 'timestamp'])\n",
    "        del df_meta\n",
    "        df = df.merge(df_wh, on=['site_id', 'timestamp'], how='left')\n",
    "        del df_wh\n",
    "        df['timestamp'] = pd.to_datetime(df.pop('timestamp'))\n",
    "        print('reduce memory')\n",
    "        df = reduce_memory_usage(df)\n",
    "        df.to_pickle(case + '.pkl')\n",
    "    return df\n",
    "    \n",
    "df_tr = read_data('train')\n",
    "df_tr.head()\n",
    "# beware to use gc to reduce the garbage collector error..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/ perform a feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO, use the following: https://www.kaggle.com/fk0728/feature-engineering-with-sklearn-pipelines/data ! Go.\n",
    "# replace below by an assiggn (for performances)\n",
    "def create_features(df): \n",
    "    df['is_weekend'] = df['timestamp'].apply(lambda x: 1 if x.date().weekday() in (5, 6) else 0)\n",
    "    df[\"weekend\"] = df[\"timestamp\"].dt.weekday.astype(np.uint8)\n",
    "    df[\"year\"] = df[\"timestamp\"].dt.year.astype(np.uint16)\n",
    "    df[\"month\"] = df[\"timestamp\"].dt.month.astype(np.uint8)\n",
    "    df[\"day\"] = df[\"timestamp\"].dt.day.astype(np.uint8)\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype(np.uint8)\n",
    "    df.drop('timestamp', axis=1, inplace=True)\n",
    "    \n",
    "    # apply labeling to 'primary_use'\n",
    "    le_dict = {'Education': 0,\n",
    "           'Office': 6,\n",
    "           'Entertainment/public assembly': 1,\n",
    "           'Lodging/residential': 4,\n",
    "           'Public services': 9,\n",
    "           'Healthcare': 3,\n",
    "           'Other': 7,\n",
    "           'Parking': 8,\n",
    "           'Manufacturing/industrial': 5,\n",
    "           'Food sales and service': 2,\n",
    "           'Retail': 11,\n",
    "           'Warehouse/storage': 15,\n",
    "           'Services': 12,\n",
    "           'Technology/science': 13,\n",
    "           'Utility': 14,\n",
    "           'Religious worship': 10}\n",
    "\n",
    "    df['primary_use'] = df['primary_use'].map(le_dict)\n",
    "    return df\n",
    "\n",
    "df_tr = create_features(df_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of meter reading with other continious variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO much more advanced feature engineering to perform !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of meter reading with categorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build pipeline and first model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO use \"primary_use\" and other variables\n",
    "categorials_to_label = ['weekend', 'month', 'day', 'meter', ]# add a \"is_holiday\"\n",
    "continuous_variables = ['air_temperature', 'cloud_coverage', 'dew_temperature', \n",
    "                        # 'precip_depth_1_hr', 'sea_level_pressure',   #do not keep these\n",
    "                       'floor_count', 'year_built', 'square_feet',\n",
    "                       'primary_use']  # already put to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sel = df_tr[categorials_to_label + continuous_variables]\n",
    "target_train = np.log1p(df_tr['meter_reading'])\n",
    "target_train = np.where(target_train < 0, 0, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# later, improve with imputation on numericals and categorials done by other variables (e.g )\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, continuous_variables),\n",
    "        ('cat', categorical_transformer, categorials_to_label)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build grid search and define hyperparameters and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "   # 'regressor__xgb__n_estimators': [100, 1000],\n",
    "   # 'regressor__xgb__learning_rate': [0.05, 0.1]\n",
    "    'lgbm__n_estimators': [100, 1000],\n",
    "    'lgbm__learning_rate': [0.1],\n",
    "    'lgbm__colsample_bytree': [0.9],\n",
    "    'lgbm__subsample': [0.9],\n",
    "    'lgbm__reg_alpha': [0.1],\n",
    "    'lgbm__reg_lambda': [0.1],\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of  10 | elapsed:  4.0min remaining:  5.9min\n",
      "[Parallel(n_jobs=4)]: Done   5 out of  10 | elapsed:  6.4min remaining:  6.4min\n",
      "[Parallel(n_jobs=4)]: Done   6 out of  10 | elapsed: 11.2min remaining:  7.5min\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  10 | elapsed: 16.1min remaining:  6.9min\n",
      "[Parallel(n_jobs=4)]: Done   8 out of  10 | elapsed: 20.1min remaining:  5.0min\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed: 29.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed: 29.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=5),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('preprocessor',\n",
       "                                        ColumnTransformer(n_jobs=None,\n",
       "                                                          remainder='drop',\n",
       "                                                          sparse_threshold=0.3,\n",
       "                                                          transformer_weights=None,\n",
       "                                                          transformers=[('num',\n",
       "                                                                         Pipeline(memory=None,\n",
       "                                                                                  steps=[('imputer',\n",
       "                                                                                          SimpleImputer(add_indicator=False,\n",
       "                                                                                                        copy=True,\n",
       "                                                                                                        fil...\n",
       "                                                      subsample_for_bin=200000,\n",
       "                                                      subsample_freq=0))],\n",
       "                                verbose=False),\n",
       "             iid=True, n_jobs=4,\n",
       "             param_grid={'lgbm__colsample_bytree': [0.9],\n",
       "                         'lgbm__learning_rate': [0.1],\n",
       "                         'lgbm__n_estimators': [100, 1000],\n",
       "                         'lgbm__reg_alpha': [0.1], 'lgbm__reg_lambda': [0.1],\n",
       "                         'lgbm__subsample': [0.9]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('lgbm', LGBMRegressor())])\n",
    "             #         ('xgb', XGBRegressor())])\n",
    "np.random.seed(47)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv = GridSearchCV(regr, param_grid, refit=True, n_jobs=4, verbose=50, cv=tscv, iid=True,\n",
    "                          scoring=('neg_mean_squared_error'))  # refer to https://scikit-learn.org/stable/modules/model_evaluation.html (neg_mean_squared_error requires not to have negative errors...)\n",
    "cv.fit(train_sel, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([156.15624223, 666.86137772]),\n",
       " 'std_fit_time': array([ 68.15666893, 251.34468369]),\n",
       " 'mean_score_time': array([ 35.66681623, 216.56990886]),\n",
       " 'std_score_time': array([ 3.42536711, 36.49043323]),\n",
       " 'param_lgbm__colsample_bytree': masked_array(data=[0.9, 0.9],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_lgbm__learning_rate': masked_array(data=[0.1, 0.1],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_lgbm__n_estimators': masked_array(data=[100, 1000],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_lgbm__reg_alpha': masked_array(data=[0.1, 0.1],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_lgbm__reg_lambda': masked_array(data=[0.1, 0.1],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_lgbm__subsample': masked_array(data=[0.9, 0.9],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'lgbm__colsample_bytree': 0.9,\n",
       "   'lgbm__learning_rate': 0.1,\n",
       "   'lgbm__n_estimators': 100,\n",
       "   'lgbm__reg_alpha': 0.1,\n",
       "   'lgbm__reg_lambda': 0.1,\n",
       "   'lgbm__subsample': 0.9},\n",
       "  {'lgbm__colsample_bytree': 0.9,\n",
       "   'lgbm__learning_rate': 0.1,\n",
       "   'lgbm__n_estimators': 1000,\n",
       "   'lgbm__reg_alpha': 0.1,\n",
       "   'lgbm__reg_lambda': 0.1,\n",
       "   'lgbm__subsample': 0.9}],\n",
       " 'split0_test_score': array([-1.94194058, -2.04684365]),\n",
       " 'split1_test_score': array([-3.50277328, -3.73417635]),\n",
       " 'split2_test_score': array([-2.38851554, -2.72115713]),\n",
       " 'split3_test_score': array([-4.28595063, -4.17732476]),\n",
       " 'split4_test_score': array([-3.86469999, -3.72606321]),\n",
       " 'mean_test_score': array([-3.196776  , -3.28111302]),\n",
       " 'std_test_score': array([0.88926431, 0.78018126]),\n",
       " 'rank_test_score': array([1, 2])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read pickle\n",
      "read data from source files\n",
      "merge datasets\n",
      "reduce memory\n",
      "Column 'row_id' converted from int64 to int32\n",
      "Column 'building_id' converted from int64 to int16\n",
      "Column 'meter' converted from int64 to int8\n",
      "Column 'site_id' converted from int64 to int8\n",
      "Column 'primary_use' converted from object to category\n",
      "Column 'square_feet' converted from int64 to int32\n",
      "Column 'year_built' converted from float64 to float32\n",
      "Column 'floor_count' converted from float64 to float32\n",
      "Column 'air_temperature' converted from float64 to float32\n",
      "Column 'cloud_coverage' converted from float64 to float32\n",
      "Column 'dew_temperature' converted from float64 to float32\n",
      "Column 'precip_depth_1_hr' converted from float64 to float32\n",
      "Column 'sea_level_pressure' converted from float64 to float32\n",
      "Column 'wind_direction' converted from float64 to float32\n",
      "Column 'wind_speed' converted from float64 to float32\n",
      "Memory usage decreased from 7848.49MB to 2584.79MB (5263.70MB, 67.07% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_test = read_data('test')\n",
    "df_test = create_features(df_test)\n",
    "test_sel = df_test[categorials_to_label + continuous_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41697600, 22)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cv.predict(test_sel)\n",
    "df_test['meter_reading'] = np.expm1(y_pred)\n",
    "df_test[['row_id', 'meter_reading']].to_csv('submission_1.csv.gz', index=False, compression='gzip')\n",
    "df_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
